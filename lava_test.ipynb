{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport gym\\nfrom lava_grid import ZigZag6x10\\nfrom agent_lava import agent\\nimport random\\n# default setting\\nmax_steps = 100\\nstochasticity = 0 # probability of the selected action failing and instead executing any of the remaining 3\\nno_render = True\\n\\nenv = ZigZag6x10(max_steps=max_steps, act_fail_prob=stochasticity, goal=(5, 9), numpy_state=False)\\ns = env.reset()\\ndone = False\\ncum_reward = 0.0\\n\\n\"\"\" Your agent\"\"\"\\nagent = agent()\\n\\n# moving costs -0.01, falling in lava -1, reaching goal +1\\n# final reward is number_of_steps / max_steps\\nwhile not done:\\n    action = agent.action()\\n    action = random.randrange(4)#: random actions\\n    ns, reward, done, _ = env.step(action)\\n    cum_reward += reward\\nprint(f\"total reward: {cum_reward}\")\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import gym\n",
    "from lava_grid import ZigZag6x10\n",
    "from agent_lava import agent\n",
    "import random\n",
    "# default setting\n",
    "max_steps = 100\n",
    "stochasticity = 0 # probability of the selected action failing and instead executing any of the remaining 3\n",
    "no_render = True\n",
    "\n",
    "env = ZigZag6x10(max_steps=max_steps, act_fail_prob=stochasticity, goal=(5, 9), numpy_state=False)\n",
    "s = env.reset()\n",
    "done = False\n",
    "cum_reward = 0.0\n",
    "\n",
    "\"\"\" Your agent\"\"\"\n",
    "agent = agent()\n",
    "\n",
    "# moving costs -0.01, falling in lava -1, reaching goal +1\n",
    "# final reward is number_of_steps / max_steps\n",
    "while not done:\n",
    "    action = agent.action()\n",
    "    action = random.randrange(4)#: random actions\n",
    "    ns, reward, done, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "print(f\"total reward: {cum_reward}\")\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ave reward -1.14\n",
      "iteration 200 ave reward -1.0090000000000003\n",
      "iteration 400 ave reward -1.0000000000000004\n",
      "iteration 600 ave reward -1.0000000000000004\n",
      "iteration 800 ave reward -1.0000000000000004\n",
      "iteration 1000 ave reward -1.0000000000000004\n",
      "iteration 1200 ave reward -1.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "from models import dqn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from lava_grid import ZigZag6x10\n",
    "from agent_lava import agent\n",
    "import random\n",
    "\n",
    "max_steps = 100\n",
    "stochasticity = 0 # probability of the selected action failing and instead executing any of the remaining 3\n",
    "no_render = True\n",
    "\n",
    "env = ZigZag6x10(max_steps=max_steps, act_fail_prob=stochasticity, goal=(5, 9), numpy_state=False)\n",
    "\n",
    "\n",
    "episodes = 3000  # number of episodes to run\n",
    "initialize = 500  # initial time steps before start updating\n",
    "\n",
    "# recieve 1 at rightmost stae and recieve small reward at leftmost state\n",
    "agent = agent(env)\n",
    "s = env.reset()\n",
    "\n",
    "rrecord = []\n",
    "totalstep = 0\n",
    "for ite in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    while not done:\n",
    "        totalstep +=1\n",
    "        action = agent.action(obs)\n",
    "\n",
    "        next_obs,reward,done,info = env.step(action)\n",
    "        if type(obs) == np.int64 :\n",
    "            obs = np.zeros((60,)).tolist()\n",
    "            #print(obs, \"test\")\n",
    "        rsum += reward\n",
    "        experience = (obs,action,reward,next_obs,done)\n",
    "        agent.buffer.append(experience)\n",
    "\n",
    "        if totalstep>initialize:\n",
    "            agent.train(totalstep)\n",
    "        obs = next_obs\n",
    "                        \n",
    "\n",
    "################################################################################\n",
    "    ## DO NOT CHANGE THIS PART!\n",
    "    rrecord.append(rsum)\n",
    "    if ite % 200 == 0:\n",
    "        print('iteration {} ave reward {}'.format(ite, np.mean(rrecord[-10:])))\n",
    "    \n",
    "    ave100 = np.mean(rrecord[-100:])   \n",
    "    if  ave100 > 17.5:\n",
    "        print(\"Solved after %d episodes.\"%ite)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b8ac3fc04880ec3c745ec33db7677d09628aa7e2e0c0a0338178b516357407b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('rl_final')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
